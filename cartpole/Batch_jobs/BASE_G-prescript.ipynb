{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "# import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# seed = int(sys.argv[1])\n",
    "seed = 9295\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "HIDDEN_LAYER        = 50\n",
    "BATCH_SIZE          = 32\n",
    "LR                  = 1e-4  # learning rate\n",
    "EPSILON             = 0.9   # greedy policy\n",
    "GAMMA               = 0.9   # reward discount\n",
    "TARGET_REPLACE_ITER = 500   # target update frequency\n",
    "MEMORY_CAPACITY     = 100000\n",
    "REWARD              = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTRA_FEAT   = 0 #masscart, masspole, length\n",
    "N_ACTIONS   = env.action_space.n \n",
    "N_STATES    = env.observation_space.shape[0] + XTRA_FEAT\n",
    "ENV_A_SHAPE = 0 if isinstance(env.action_space.sample(), int) else env.action_space.sample().shape     # to confirm the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(N_STATES, HIDDEN_LAYER)\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight)\n",
    "\n",
    "        self.adv = nn.Linear(HIDDEN_LAYER, N_ACTIONS)\n",
    "        nn.init.xavier_uniform_(self.adv.weight) \n",
    "    \n",
    "        self.val = nn.Linear(HIDDEN_LAYER, 1)\n",
    "        nn.init.xavier_uniform_(self.val.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        adv = self.adv(x)\n",
    "        val = self.val(x)\n",
    "        \n",
    "        return val + adv - adv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D3QN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "#         print(\"Neural net\")\n",
    "#         print(self.eval_net)\n",
    "\n",
    "        self.learn_step_counter  = 0 # for target updating\n",
    "        self.memory_counter      = 0\n",
    "        self.memory              = np.zeros((int(MEMORY_CAPACITY), N_STATES * 2 + 2)) # initialize memory\n",
    "        \n",
    "#         self.good_memory_counter = 0 # for storing non-terminal memories\n",
    "#         self.good_memory         = np.zeros((int(MEMORY_CAPACITY/2), N_STATES * 2 + 2)) # initialize memory\n",
    "        \n",
    "#         self.bad_memory_counter  = 0 # for storing terminal memories\n",
    "#         self.bad_memory          = np.zeros((int(MEMORY_CAPACITY/2), N_STATES * 2 + 2)) # initialize memory\n",
    "        \n",
    "        self.optimizer           = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
    "        self.loss_func           = nn.MSELoss()\n",
    "        \n",
    "        self.running_loss        = 0\n",
    "        self.loss_rec            = 0 \n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
    "        return action\n",
    "    \n",
    "    def choose_greedy_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        # input only one sample\n",
    "        actions_value = self.eval_net.forward(x)\n",
    "        action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "        action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        index = self.memory_counter % int(MEMORY_CAPACITY)\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "        \n",
    "#         if r > 0: #non-terminal rewards\n",
    "#             # replace the old memory with new memory\n",
    "#             index = self.good_memory_counter % int(MEMORY_CAPACITY/2)\n",
    "#             self.good_memory[index, :] = transition\n",
    "#             self.good_memory_counter += 1\n",
    "        \n",
    "#         else: #terminal rewards\n",
    "#             # replace the old memory with new memory\n",
    "#             index = self.bad_memory_counter % int(MEMORY_CAPACITY/2)\n",
    "#             self.bad_memory[index, :] = transition\n",
    "#             self.bad_memory_counter += 1\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "        \n",
    "        sample_index_limit = min(MEMORY_CAPACITY, self.memory_counter)\n",
    "        sample_index = np.random.choice(int(sample_index_limit),int(BATCH_SIZE))\n",
    "        b_memory = self.memory[sample_index,:]\n",
    "        # sample batch transitions\n",
    "#         good_sample_index_limit = min(MEMORY_CAPACITY/2, self.good_memory_counter)\n",
    "#         bad_sample_index_limit  = min(MEMORY_CAPACITY/2, self.bad_memory_counter)\n",
    "        \n",
    "#         good_sample_index = np.random.choice(int(good_sample_index_limit), int(BATCH_SIZE-int(BATCH_SIZE*TERMINAL_BIAS)))\n",
    "#         bad_sample_index  = np.random.choice(int(bad_sample_index_limit),  int(BATCH_SIZE*TERMINAL_BIAS))\n",
    "\n",
    "#         b_good_memory = self.good_memory[good_sample_index, :]\n",
    "#         b_bad_memory  = self.bad_memory[bad_sample_index, :]\n",
    "#         b_memory      = np.vstack((b_good_memory,b_bad_memory))\n",
    "        \n",
    "        b_s  = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a  = torch.LongTensor( b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        b_r  = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval   = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        a_eval   = self.eval_net(b_s).max(1)[1].view(BATCH_SIZE, 1) #best action according to eval_net\n",
    "        q_next   = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.gather(1, a_eval)   # shape (batch, 1)\n",
    "        loss     = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.running_loss += loss.mean().item()\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.loss_rec = np.append(self.loss_rec, self.running_loss / TARGET_REPLACE_ITER)\n",
    "            self.running_loss = 0.0\n",
    "#         self.running_loss += loss.mean().item()\n",
    "#         if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "#             print('EPISODE: ', i_episode, 'LOSS : %.3f' %(self.running_loss / TARGET_REPLACE_ITER))\n",
    "#             self.running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = D3QN()\n",
    "NO_OF_EPISODES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 9295 REWARD: 100 FAILURE "
     ]
    }
   ],
   "source": [
    "# print('\\nCollecting experience...')\n",
    "TIMESTEP_LIMIT = 200\n",
    "total_time_steps = 0\n",
    "time_rec = []\n",
    "learn_start_flag = True\n",
    "SOLVE_METRIC = 195\n",
    "SOLVE = False\n",
    "state_max  = np.ones(N_STATES)*(-100)\n",
    "state_min  = np.ones(N_STATES)*(100)\n",
    "for i_episode in range(NO_OF_EPISODES):\n",
    "    s = env.reset()\n",
    "    ep_r = 0\n",
    "    time_steps = 0\n",
    "    while True:\n",
    "#         env.render()\n",
    "        state_max = np.maximum(state_max,s) \n",
    "        state_min = np.minimum(state_min,s)\n",
    "        time_steps += 1\n",
    "        total_time_steps += 1\n",
    "        a = dqn.choose_action(s)\n",
    "\n",
    "        # take action\n",
    "        s_, r, done, info = env.step(a)\n",
    "        if done:\n",
    "            if time_steps >= TIMESTEP_LIMIT:\n",
    "                r = REWARD\n",
    "            else:\n",
    "                r = -REWARD\n",
    "           \n",
    "        dqn.store_transition(s, a, r, s_)\n",
    "        \n",
    "        if total_time_steps >= 2000:\n",
    "            dqn.learn()\n",
    "            if learn_start_flag:\n",
    "#                 print(\"Learning starts from EPISODE: \",i_episode)\n",
    "                learn_start_flag = False\n",
    "        \n",
    "        if done:\n",
    "            time_rec = np.append(time_rec, time_steps)\n",
    "            break\n",
    "        s = s_\n",
    "    \n",
    "    #if minimum of episode length of last 100 episodes is greater than upgrade_metric=195\n",
    "    if time_rec[-100:].mean() >= SOLVE_METRIC:\n",
    "        SOLVE = True\n",
    "        print(\"SEED:\",seed,\"REWARD:\",REWARD, \"SUCCESS @ EP: \", i_episode , end=\" \")\n",
    "        break\n",
    "\n",
    "if not SOLVE:\n",
    "    print(\"SEED:\",seed,\"REWARD:\",REWARD, \"FAILURE\", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Space Exploration:\n",
      "[1.03710017 1.9734055  0.20933099 1.69542267] [-0.26901063 -0.97563549 -0.20943485 -2.9514836 ] "
     ]
    }
   ],
   "source": [
    "# print(\"State Space Exploration:\")\n",
    "print(state_max, end=\" \")\n",
    "print(state_min, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WINDOW = 10\n",
    "# lossavg = running_mean(dqn.loss_rec, WINDOW)\n",
    "# plt.plot(dqn.loss_rec,alpha=0.5,color='g')\n",
    "# plt.plot(lossavg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WINDOW = 30\n",
    "# ravg = running_mean(time_rec, WINDOW)\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize = (15,6))\n",
    "\n",
    "# ax2 = fig.add_subplot(1, 1, 1)\n",
    "# data = time_rec\n",
    "# ax2.plot(data, color = 'g',alpha=0.5)\n",
    "# ax2.plot(np.ones_like(data)*1500, 'g--')\n",
    "# ax2.plot(np.ones_like(data)*200, 'r--')\n",
    "\n",
    "# ax2.set_xlabel('Iterations',color = 'k')\n",
    "# ax2.set_ylabel('Time Steps',color = 'g')\n",
    "# ax2.set_ylim([1,2.5e2])\n",
    "# fig.tight_layout()\n",
    "# ax2.grid()\n",
    "\n",
    "\n",
    "# plt.plot(ravg)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/BASE_G_21_18_07\n"
     ]
    }
   ],
   "source": [
    "MODELNAME = './models/BASE_G' + datetime.now().strftime(\"_%H_%M_%S\")\n",
    "# print(MODELNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dqn.eval_net.state_dict(), MODELNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL TIMESTEPS:  48332\n"
     ]
    }
   ],
   "source": [
    "print(\"TOTAL TIMESTEPS: \", total_time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ttest_time_rec = []\n",
    "# for i_episode in range(1000):\n",
    "# #     xtra = [env.length]\n",
    "#     s = env.reset()\n",
    "# #     s = np.append(s, xtra)\n",
    "#     time_steps = 0\n",
    "#     while True:\n",
    "# #         env.render()\n",
    "#         time_steps += 1\n",
    "#         a = dqn.choose_greedy_action(s)\n",
    "\n",
    "#         # take action\n",
    "#         s_, r, done, info = env.step(a)\n",
    "# #         s_ = np.append(s_, xtra)\n",
    "\n",
    "#         if done or time_steps >= TIMESTEP_LIMIT:\n",
    "#             ttest_time_rec = np.append(ttest_time_rec, time_steps)\n",
    "#             break\n",
    "#         s = s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize = (15,6))\n",
    "\n",
    "# ax2 = fig.add_subplot(1, 1, 1)\n",
    "# data = ttest_time_rec\n",
    "# ax2.plot(data, color = 'g')\n",
    "# ax2.plot(np.ones_like(data)*1500, 'g--')\n",
    "# ax2.plot(np.ones_like(data)*200, 'r--')\n",
    "\n",
    "# ax2.set_xlabel('Iterations',color = 'k')\n",
    "# ax2.set_ylabel('Time Steps',color = 'g')\n",
    "# ax2.set_ylim([1,2.5e2])\n",
    "# fig.tight_layout()\n",
    "# ax2.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
